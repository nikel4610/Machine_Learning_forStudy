{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysyrghi1QKqk"
      },
      "source": [
        "# 13장 모델의 성능 검증하기\n",
        "\n",
        "1986년 제프리 힌튼 교수가 오차 역전파를 발표한 직후, 존스 홉킨스의 세즈노프스키(Sejnowski) 교수는 오차 역전파가 은닉층의 가중치를 실제로 업데이트시키는 것을 확인하고 싶었습니다. 그는 **광석과 일방 암석에 수중 음파 탐지기를 쏜 후 결과를 모아 데이터셋을 준비했고 음파 탐지기의 수신 결과만 보고 광석인지 일반 암석인지를 구부하는 모델을 만들었습니다.** 그가 측정한 결과의 정확도는 얼마였을까요?\n",
        "\n",
        "<br><center>\n",
        "<img src=\"https://drive.google.com/uc?id=1G-cPV2ET-IrDbdzQFh430UCMz7CHuPpL\" width=400>\n",
        "</center><br>\n",
        "\n",
        "\n",
        "이 장에서는 세즈노프스키 교수가 했던 초음파 광물 예측 실험을 텐서플로로 재현해보고 이렇게 구해진 실험 정확도를 평가하는 방법과 성능을 향상시키는 중요한 머신 러닝 기법들에 대해 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmMTfSRXQKqp"
      },
      "source": [
        "##  1. 데이터의 확인과 예측 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "3k7ysgDLQKqr",
        "outputId": "3e7b0bee-8b8f-4cd6-a507-0e79a35dcbfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'data' already exists and is not an empty directory.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c043d488-b33c-41d7-b0d0-60d55d0de782\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c043d488-b33c-41d7-b0d0-60d55d0de782')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c043d488-b33c-41d7-b0d0-60d55d0de782 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c043d488-b33c-41d7-b0d0-60d55d0de782');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0       1       2       3       4       5       6       7       8   \\\n",
              "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
              "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
              "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
              "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
              "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
              "\n",
              "       9   ...      51      52      53      54      55      56      57  \\\n",
              "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
              "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
              "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
              "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
              "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
              "\n",
              "       58      59  60  \n",
              "0  0.0090  0.0032   0  \n",
              "1  0.0052  0.0044   0  \n",
              "2  0.0095  0.0078   0  \n",
              "3  0.0040  0.0117   0  \n",
              "4  0.0107  0.0094   0  \n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 깃허브에 준비된 데이터를 가져옵니다.\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "# 광물 데이터를 불러옵니다.\n",
        "df = pd.read_csv('./data/sonar3.csv', header=None)\n",
        "\n",
        "# 첫 5줄을 봅니다. \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5CpPvtSySrA"
      },
      "source": [
        "첫번째 열(0)부터 60번째(59)열까지는 음파의 에너지를 0에서 1 사이의 숫자로 표시하고 있습니다. 이제 일반 암서과 광석이 각각 몇 개나 데이터셋에 포함되어 있는지 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKPI6eI2QKqs",
        "outputId": "3d803dc4-1a09-41f0-e5f4-9da90218505f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    111\n",
              "0     97\n",
              "Name: 60, dtype: int64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 일반 암석(0)과 광석(1)이 몇 개 있는지 확인합니다.\n",
        "df[60].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m52xFrSVx8ZV"
      },
      "source": [
        "광석 샘플이 111개 암석 샘플이 97개 따라서 샘플 수는 총 111+97= 208개의 샘플이 데이터 셋을 구성하고 있습니다. \n",
        "\n",
        "1\\~60번째(0~59) 열을 변수 $X$에 저장하고 광물의 종류를 $y$로 표현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI7lJCZ6QKqs"
      },
      "outputs": [],
      "source": [
        "# 음파 관련 속성을 X로, 광물의 종류를 y로 저장합니다.\n",
        "X = df.iloc[:,0:60]\n",
        "y = df.iloc[:,60]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi4AplhWzHog"
      },
      "source": [
        "이후 앞서 했던 그대로 딥러닝을 실행하겠습니다. 출력 $y$는 하나이며 은닉층..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s65Y52a4QKqu",
        "outputId": "4613f1fb-bd95-44d2-e755-bf035cb62019",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 2ms/step - loss: 0.6947 - accuracy: 0.4615\n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.6833 - accuracy: 0.5385\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.6730 - accuracy: 0.5625\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.6613 - accuracy: 0.6106\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.6298\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.6587\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.6267 - accuracy: 0.6779\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.6070 - accuracy: 0.7067\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.7067\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.5643 - accuracy: 0.7644\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.5401 - accuracy: 0.7788\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7837\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.8029\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.8077\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7933\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.7981\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.7740\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.8269\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8029\n",
            "Epoch 20/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4252 - accuracy: 0.8365\n",
            "Epoch 21/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4152 - accuracy: 0.8317\n",
            "Epoch 22/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8173\n",
            "Epoch 23/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8221\n",
            "Epoch 24/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8125\n",
            "Epoch 25/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8221\n",
            "Epoch 26/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8413\n",
            "Epoch 27/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8365\n",
            "Epoch 28/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.8317\n",
            "Epoch 29/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3748 - accuracy: 0.8317\n",
            "Epoch 30/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3844 - accuracy: 0.8317\n",
            "Epoch 31/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3588 - accuracy: 0.8702\n",
            "Epoch 32/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3589 - accuracy: 0.8558\n",
            "Epoch 33/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8413\n",
            "Epoch 34/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3597 - accuracy: 0.8413\n",
            "Epoch 35/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8365\n",
            "Epoch 36/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8558\n",
            "Epoch 37/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8510\n",
            "Epoch 38/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8365\n",
            "Epoch 39/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.8750\n",
            "Epoch 40/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8702\n",
            "Epoch 41/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8846\n",
            "Epoch 42/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8654\n",
            "Epoch 43/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.8702\n",
            "Epoch 44/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8750\n",
            "Epoch 45/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.8846\n",
            "Epoch 46/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8990\n",
            "Epoch 47/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8654\n",
            "Epoch 48/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2930 - accuracy: 0.8846\n",
            "Epoch 49/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2927 - accuracy: 0.8846\n",
            "Epoch 50/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.3154 - accuracy: 0.8654\n",
            "Epoch 51/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.9038\n",
            "Epoch 52/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.9135\n",
            "Epoch 53/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.9231\n",
            "Epoch 54/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8942\n",
            "Epoch 55/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2803 - accuracy: 0.8750\n",
            "Epoch 56/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.9183\n",
            "Epoch 57/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.9279\n",
            "Epoch 58/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2741 - accuracy: 0.9183\n",
            "Epoch 59/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.9135\n",
            "Epoch 60/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2604 - accuracy: 0.9135\n",
            "Epoch 61/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.9279\n",
            "Epoch 62/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.9279\n",
            "Epoch 63/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9375\n",
            "Epoch 64/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2437 - accuracy: 0.9183\n",
            "Epoch 65/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2513 - accuracy: 0.9135\n",
            "Epoch 66/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2392 - accuracy: 0.9279\n",
            "Epoch 67/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9231\n",
            "Epoch 68/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9038\n",
            "Epoch 69/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.9087\n",
            "Epoch 70/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.9279\n",
            "Epoch 71/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9375\n",
            "Epoch 72/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9327\n",
            "Epoch 73/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2232 - accuracy: 0.9279\n",
            "Epoch 74/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.9327\n",
            "Epoch 75/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9279\n",
            "Epoch 76/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2101 - accuracy: 0.9231\n",
            "Epoch 77/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2132 - accuracy: 0.9279\n",
            "Epoch 78/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9135\n",
            "Epoch 79/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1940 - accuracy: 0.9567\n",
            "Epoch 80/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9327\n",
            "Epoch 81/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1987 - accuracy: 0.9423\n",
            "Epoch 82/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.9423\n",
            "Epoch 83/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1926 - accuracy: 0.9375\n",
            "Epoch 84/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9231\n",
            "Epoch 85/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1825 - accuracy: 0.9375\n",
            "Epoch 86/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1836 - accuracy: 0.9471\n",
            "Epoch 87/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9519\n",
            "Epoch 88/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1778 - accuracy: 0.9471\n",
            "Epoch 89/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1732 - accuracy: 0.9519\n",
            "Epoch 90/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1699 - accuracy: 0.9519\n",
            "Epoch 91/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1687 - accuracy: 0.9519\n",
            "Epoch 92/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1640 - accuracy: 0.9519\n",
            "Epoch 93/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1643 - accuracy: 0.9567\n",
            "Epoch 94/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1587 - accuracy: 0.9615\n",
            "Epoch 95/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.9663\n",
            "Epoch 96/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1501 - accuracy: 0.9615\n",
            "Epoch 97/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1577 - accuracy: 0.9519\n",
            "Epoch 98/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1631 - accuracy: 0.9423\n",
            "Epoch 99/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1541 - accuracy: 0.9519\n",
            "Epoch 100/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9615\n",
            "Epoch 101/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1405 - accuracy: 0.9663\n",
            "Epoch 102/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1412 - accuracy: 0.9615\n",
            "Epoch 103/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9663\n",
            "Epoch 104/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1365 - accuracy: 0.9712\n",
            "Epoch 105/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.9663\n",
            "Epoch 106/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1298 - accuracy: 0.9712\n",
            "Epoch 107/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1390 - accuracy: 0.9615\n",
            "Epoch 108/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9663\n",
            "Epoch 109/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.9712\n",
            "Epoch 110/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1249 - accuracy: 0.9712\n",
            "Epoch 111/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.9856\n",
            "Epoch 112/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9760\n",
            "Epoch 113/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1192 - accuracy: 0.9760\n",
            "Epoch 114/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9760\n",
            "Epoch 115/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1133 - accuracy: 0.9712\n",
            "Epoch 116/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9760\n",
            "Epoch 117/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9808\n",
            "Epoch 118/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1048 - accuracy: 0.9856\n",
            "Epoch 119/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1039 - accuracy: 0.9760\n",
            "Epoch 120/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1018 - accuracy: 0.9808\n",
            "Epoch 121/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9760\n",
            "Epoch 122/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1103 - accuracy: 0.9712\n",
            "Epoch 123/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9856\n",
            "Epoch 124/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1024 - accuracy: 0.9760\n",
            "Epoch 125/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9856\n",
            "Epoch 126/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0916 - accuracy: 0.9808\n",
            "Epoch 127/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.1019 - accuracy: 0.9760\n",
            "Epoch 128/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0898 - accuracy: 0.9904\n",
            "Epoch 129/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0894 - accuracy: 0.9808\n",
            "Epoch 130/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0896 - accuracy: 0.9904\n",
            "Epoch 131/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9808\n",
            "Epoch 132/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 0.9904\n",
            "Epoch 133/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9808\n",
            "Epoch 134/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0814 - accuracy: 0.9856\n",
            "Epoch 135/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0825 - accuracy: 0.9808\n",
            "Epoch 136/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.9904\n",
            "Epoch 137/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9856\n",
            "Epoch 138/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9808\n",
            "Epoch 139/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9904\n",
            "Epoch 140/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9904\n",
            "Epoch 141/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9904\n",
            "Epoch 142/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9904\n",
            "Epoch 143/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9856\n",
            "Epoch 144/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9808\n",
            "Epoch 145/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0641 - accuracy: 0.9952\n",
            "Epoch 146/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0659 - accuracy: 0.9904\n",
            "Epoch 147/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0644 - accuracy: 0.9952\n",
            "Epoch 148/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0700 - accuracy: 0.9808\n",
            "Epoch 149/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9952\n",
            "Epoch 150/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9952\n",
            "Epoch 151/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9904\n",
            "Epoch 152/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9952\n",
            "Epoch 153/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0580 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9952\n",
            "Epoch 155/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.0681 - accuracy: 0.9904\n",
            "Epoch 156/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9952\n",
            "Epoch 157/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0532 - accuracy: 0.9952\n",
            "Epoch 158/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9904\n",
            "Epoch 159/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0531 - accuracy: 0.9952\n",
            "Epoch 160/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9952\n",
            "Epoch 161/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9952\n",
            "Epoch 162/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9952\n",
            "Epoch 164/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.0457 - accuracy: 0.9952\n",
            "Epoch 165/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0435 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 0.9904\n",
            "Epoch 169/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9952\n",
            "Epoch 171/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0465 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9952\n",
            "Epoch 174/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0403 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.9952\n",
            "Epoch 177/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0233 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 모델을 설정합니다.\n",
        "model = Sequential()\n",
        "model.add(Dense(24,  input_dim=60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델을 실행합니다.\n",
        "history=model.fit(X, y, epochs=200, batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24g-GJ1RzfUN"
      },
      "source": [
        "200번 반복되었을 때의 결과를 보니 정확도가 100%입니다. 이 모델의 예측 정확도가 100%라는 것을 믿을 수 있습니까? 정말로 광석인지 일반 암석인지  100%의 확율로 판별해 내는 모델이 만들어진 것이라요? 다음 섹션에서 이 의문에 대한 답을 찾아 보겠습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7dkVfumzzm2"
      },
      "source": [
        "## 2. 과적합 이해하기\n",
        "이제 과적합 문제가 무엇인지 알아보고 이를 어떻게 해결하는지 살펴보겠습니다. 과적합(overfitting)이란 모델이 학습 데이터셋 안에서는 일정 수준 이상의 예측 정보를 보이지만 새로운 데이터에 적용하면 잘 맞지 않는 것을 의미합니다. \n",
        "\n",
        "그림(13-1)의 그래프에는 두 종류의 데이터가 있습니다. 원 안이 검은 색인 것과 원 안이 흰색인 두 종류의 데이터가 보입니다. 이 두 종류를 완벽하게 또는 매우 정확하게 분류하기 위해 구분 선을 심한 곡선으로 그린 것(과적합)과 그 외 두 가지의 직선 행태의 구분선이 보입니다.   \n",
        "<br><center>\n",
        "(그림. 13-1) 과적합<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1bPEOjpG1MqG1eF_WSZjHwDUpczsAnAU2\" width=500>\n",
        "</center><br>  \n",
        "\n",
        "과적합 구분 선은 주어진 샘플들에만 최적화 되어 있습니다. 새로운 데이터가 주어졌을 때 과적합된, 그러니까 기존 데이터에 overfitting된 구분선으로 새로운 데이터를 정확히 불류하기 어렵다는 것입니다. \n",
        "\n",
        "과적합(overfitting)은 층이 너무 많거나 변수가 복자해서 살생하기도 하고 테스트셋과 학습셋이 중복될 때 생기기도 합니다. 특히 딥러닝은 학습 단계에서 입력층, 은닉층, 출력층의 노드들에 상당히 많은 변수가 투입됩니다. 따라서 딥러닝을 진행하는 동안 과적합에 빠지 않게 늘 주의해야 합니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9pTY7GFQKqv"
      },
      "source": [
        "## 3. 학습셋과 테스트셋\n",
        "그렇다면 과적합을 방지하려면 어떻게 해야할까요? 먼저 학습을 하는 데이터셋과 이를 테스트할 데이터셋을 완전히 구분한 후 학습과 동시에 테스트를 변행하며 진행하는 것이 한 방법입니다. 예를 들어 데이터세이 총 100개의 샘플로 이루어져 있다면 다음과 같이 두 개의 셋으로 나눕니다. \n",
        "\n",
        "<br><center>\n",
        "(그림. 13-2) 학습셋과 테스트셋 구분<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1MBgSLj0CodjyAHGZx4o4EIvKDFLolpM1\">\n",
        "</center><br>\n",
        "\n",
        "(전체 데이터셋의 70\\~75%를 학습셋으로 사용하고 30\\~25%를 테스트 셋으로 나눕니다.) 신경망을 만들어 70개의 샘플로 학습을 진행한 후 이 학습의 결과를 저장합니다. 이렇게 저당된 파일을 '모델'이라고 합니다. 모델은 다른 셋에 적용할 경우 학습 단계에서 각인되었던 그대로 다시 수행합니다. 따라서 나머지 30개의 샘플로 테스트해서 정확도를 살펴보면 학습이 얼마나 잘 되었는지 알 수 있을 것입니다. 딥러닝 같은 알고리즘을 충분히 조절해 가장 나은 모델이 만들어지면 이를 실생활에 대입해 활용하는 것이 바로 머신 러닝의 개발 순서입니다.  \n",
        "\n",
        "<br><center>\n",
        "(그림. 13-3) 학습셋과 테스트셋<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1BOHkL4fJVuVrCdIpfC9P02t-TUB8_f92\" width=400>\n",
        "</center><br>  \n",
        "지금까지 우리는 테스트셋을 만들지 않고 모든 데이터셋을 이용해 학습시켰습니다. 그런데로 매번 정확도(accuracy)를 계산할 수 있었지요. 어떻게 가능했을까요?  \n",
        "\n",
        "**\"지금까지 학습데이터를 이용해 정확도를 측정한 것은 데이터셋에 들어있는 모든 샘플을 그대로 테스트에 활용한 결과입니다.\" → 지금까지 모든 데이터셋을 이용해 학습했고 모든 데이터셋(데이터 샘플)을 가지고 테스트한 결과입니다.** \n",
        "\n",
        "이를 통해 학습이 진행되는 상황을 파악할 수는 있지만 새로운 데이터에 적용했을 때 어느 정도의 성능이 나올지 알수 없습니다. 머신 러닝의 최종 목적은 과거의 데이터를 토대로 새로운 데이터를 예측하는 것입니다. 즉, 새로운 데이터에 사용할 모델을 만드는 것이 최종 목적이므로 테스트셋을 만들어 정확한 평가를 병행하는 것이 매우 중요합니다. \n",
        "\n",
        "학습셋만 가지고 평가할 때, 층을 더하거나 에포크(epoch) 값을 높여 실행 횟수를르리면 정확도가 계소해서 올라갈 수 있습니다. 하지만 학습 데이터셋만으로 평가한 예측 성공률이 테스트셋에서도 그래도 나나타지는 않습니다. 즉, 학습이 깊어져 학습셋 내부에서 성공률은 높아져도 테스트셋에서는 효과가 없다면 과적합이 일어난 것이지요. 이를 그래프로 표현하면 그림(13-4)와 같습니다.\n",
        "\n",
        "<br><center>\n",
        "(그림. 13-4) 학습이 계속되면 학습셋에서는 에러는 계속해서 작아져 과적합 발생<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1ZrnHjrDC6TXjPww9R5JFyYrwHrDyNIME\" width=400>\n",
        "</center><br>  \n",
        "\n",
        "학습을 진행해도 테스트 결과가 더 이상 좋아지지 않는 시점에서 학습을 멈춰야 합니다. 이때 학습 정도가 가장 적절한 것으로 볼 수 있습니다. \n",
        "\n",
        "우리가 다루는 초음파 광물 예측 모델을 만든 세즈노프스키 교수가 실험 결과를 발표한 논무의 일부를 가져와 보겠습니다. \n",
        "\n",
        "<br><center>\n",
        "(그림. 13-5) 학습셋과 테스트셋 정확도 측정의 예(RP Gorman et.al., 1998)<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1pnJsvvIZ_RJwDhyQ63nGsgOIKofvPv9L\">\n",
        "</center><br>  \n",
        "\n",
        "여기서 눈여겨보아야 할 부분은 은닉층(Number of Hidden Units) 개수가 올라감에 따라 학습셋의 예측율(Average Performacne on Training Sets)과 데스트셋의 예측률(Average Performance on Testing Sets)이 어떻게 변하는지입니다. 이 부분만 따로  뽑아서 정리하면 표(13-2)와 같습니다. \n",
        "\n",
        "<br><center>\n",
        "(표. 13-2) 은닉층 개수의 변화에 따른 학습셋의 예측률<br>\n",
        "\n",
        "|은닉층 개수|학습셋의 예측률|테스트셋의 예측율|\n",
        "|:---:|---:|---:|\n",
        "|0|79.3|73.1|\n",
        "|2|96.2|85.7|\n",
        "|3|98.1|87.6|\n",
        "|6|99.4|89.3|\n",
        "|12|99.8|90.4|\n",
        "|24|100|89.2|\n",
        "\n",
        "</center><br>  \n",
        "\n",
        "은닉층이 늘어날수록 학습셋의 에측률이 점점 올라가다가 결국 24개 층에 이르면 100% 예측률을 보입니다. 우리가 조금 전에 실행했던 결과와 같습니다. 그런데 이 모델을 토대로 테스트한 결과는 어떤가요? 테스트셋 예측률은 은닉층의 개수가 12개일 때 90.4%로 최고를 이루다 24개째에서는 다시 89.2%로 떨어지고 맙니다. 즉, 식이 복잡해지고 학습량이 늘어날수록 학습 데이터를 통한 예측률은 계속해서 올라가지만 은닉층의 수를 적절하게 조절하지 않을 경우 테스트셋을 이용한 예측률은 오히려 떨어지는 것을 확인할 수 있습니다. \n",
        "\n",
        "그러면 예제에 주어진 데이터를 학습 데이터셋과 테스트셋으로 나누는 예제를 만들어 보겠습니다. \n",
        "\n",
        "🚀 여기서 잠깐.  \n",
        "이 실습에서는 사이킷런(scikit-learn) 라이브러리가 필요합니다.\n",
        "\n",
        "저장된 $X$ 데이터와 $y$ 데이터에서 각각 정해진 비율(%)만큼 학습 데이터셋과 테스트 데이터셋으로 분리시키는 함수가 사이킷런의 ```train_test_split()```함수입니다. 따라서 다음과 같이 학습 데이터셋과 테스트 데이터셋을 만들 수 있습니다. 총 데이터셋에서 학습 데이터셋을 70%, 테스트 데이터셋을 30%로 나눌 때의 코드  \n",
        "예입니다. \n",
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, shuffle=True)\n",
        "```\n",
        "위 코드에서 ```test_size```은 테스트 데이터셋의 비율입니다. 0.3은 전체 데이터셋의 30%를 테스트 데이터셋으로 사용하겠다는 것으로 나머지 70%를 학습 데이터셋으로 사용하게 됩니다. 이렇게 나누어진 학습 데이터셋과 테스트 데이터셋으로 각각 ```X_train, y_train``` 그리고 ```X_test, y_test```에 저장됩니다. \n",
        "\n",
        "모델은 앞서 만든 구조를 그대로 유지하고 모델에 성능 평가를 위해 테스트 함수(```model.evaluate()```)를 추가했습니다. \n",
        "\n",
        "```\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', score[1])\n",
        "```\n",
        "```model.evaluate()``` 함수는 loss와 accuracy 두 가지를 계산해 출력합니다. 이를 score에 저장하고 accuracy를 출력하도록 했습니다. \n",
        "\n",
        "이제 전체 코드를 실행해 보겠습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXNlEThdQKqv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMwZzxk0-We4",
        "outputId": "d697ea2d-6a8f-4704-ad0a-7c2e43055281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'data' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# 깃허브에 준비된 데이터를 가져옵니다. 앞에서 이미 데이터를 가져왔으므로 추석 처리합니다. 3번 예제만 별도 실행 시 주석을 해제하여 실습하세요.\n",
        "!git clone https://github.com/taehojo/data.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf1oIhQiQKqw"
      },
      "outputs": [],
      "source": [
        "# 광물 데이터를 불러옵니다.\n",
        "df = pd.read_csv('./data/sonar3.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv0lrt18QKqw"
      },
      "outputs": [],
      "source": [
        "# 음파 관련 속성을 X로, 광물의 종류를 y로 저장합니다.\n",
        "X = df.iloc[:,0:60]\n",
        "y = df.iloc[:,60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KVHOH9MQKqw"
      },
      "outputs": [],
      "source": [
        "# 학습셋과 테스트셋을 구분합니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLLIgz4uQKqx",
        "outputId": "f2dd52e0-71a2-4c73-b633-bca1824a2e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5103\n",
            "Epoch 2/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6763 - accuracy: 0.5724\n",
            "Epoch 3/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6651 - accuracy: 0.5793\n",
            "Epoch 4/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6553 - accuracy: 0.6345\n",
            "Epoch 5/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6451 - accuracy: 0.6552\n",
            "Epoch 6/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.6828\n",
            "Epoch 7/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.6690\n",
            "Epoch 8/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6142 - accuracy: 0.7172\n",
            "Epoch 9/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6028 - accuracy: 0.6966\n",
            "Epoch 10/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5901 - accuracy: 0.7103\n",
            "Epoch 11/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.7379\n",
            "Epoch 12/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.7517\n",
            "Epoch 13/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7724\n",
            "Epoch 14/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7793\n",
            "Epoch 15/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7931\n",
            "Epoch 16/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7931\n",
            "Epoch 17/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.8000\n",
            "Epoch 18/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8138\n",
            "Epoch 19/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.8207\n",
            "Epoch 20/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.8069\n",
            "Epoch 21/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8345\n",
            "Epoch 22/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8207\n",
            "Epoch 23/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.8621\n",
            "Epoch 24/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4106 - accuracy: 0.8483\n",
            "Epoch 25/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4109 - accuracy: 0.8483\n",
            "Epoch 26/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8207\n",
            "Epoch 27/200\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.3830 - accuracy: 0.8759\n",
            "Epoch 28/200\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3755 - accuracy: 0.8897\n",
            "Epoch 29/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8552\n",
            "Epoch 30/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3751 - accuracy: 0.8483\n",
            "Epoch 31/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3634 - accuracy: 0.8552\n",
            "Epoch 32/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3531 - accuracy: 0.8897\n",
            "Epoch 33/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8552\n",
            "Epoch 34/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8828\n",
            "Epoch 35/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8759\n",
            "Epoch 36/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3261 - accuracy: 0.8621\n",
            "Epoch 37/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8621\n",
            "Epoch 38/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.8690\n",
            "Epoch 39/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.8828\n",
            "Epoch 40/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3086 - accuracy: 0.9034\n",
            "Epoch 41/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.8897\n",
            "Epoch 42/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.8897\n",
            "Epoch 43/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8552\n",
            "Epoch 44/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2900 - accuracy: 0.9103\n",
            "Epoch 45/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8690\n",
            "Epoch 46/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2839 - accuracy: 0.9034\n",
            "Epoch 47/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.8897\n",
            "Epoch 48/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2726 - accuracy: 0.9034\n",
            "Epoch 49/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.8897\n",
            "Epoch 50/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.8966\n",
            "Epoch 51/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8759\n",
            "Epoch 52/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.8966\n",
            "Epoch 53/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.9103\n",
            "Epoch 54/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.8897\n",
            "Epoch 55/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.9103\n",
            "Epoch 56/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8966\n",
            "Epoch 57/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9034\n",
            "Epoch 58/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2469 - accuracy: 0.9034\n",
            "Epoch 59/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.8828\n",
            "Epoch 60/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2369 - accuracy: 0.9241\n",
            "Epoch 61/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2404 - accuracy: 0.8966\n",
            "Epoch 62/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9172\n",
            "Epoch 63/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9172\n",
            "Epoch 64/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.9034\n",
            "Epoch 65/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9103\n",
            "Epoch 66/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9241\n",
            "Epoch 67/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2058 - accuracy: 0.9241\n",
            "Epoch 68/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.9379\n",
            "Epoch 69/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9241\n",
            "Epoch 70/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9310\n",
            "Epoch 71/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.9241\n",
            "Epoch 72/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1937 - accuracy: 0.9172\n",
            "Epoch 73/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1953 - accuracy: 0.9517\n",
            "Epoch 74/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1850 - accuracy: 0.9241\n",
            "Epoch 75/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2129 - accuracy: 0.9379\n",
            "Epoch 76/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1936 - accuracy: 0.9310\n",
            "Epoch 77/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1817 - accuracy: 0.9517\n",
            "Epoch 78/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1739 - accuracy: 0.9448\n",
            "Epoch 79/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1678 - accuracy: 0.9379\n",
            "Epoch 80/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1650 - accuracy: 0.9517\n",
            "Epoch 81/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1644 - accuracy: 0.9448\n",
            "Epoch 82/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1593 - accuracy: 0.9586\n",
            "Epoch 83/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1571 - accuracy: 0.9310\n",
            "Epoch 84/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1615 - accuracy: 0.9655\n",
            "Epoch 85/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1629 - accuracy: 0.9310\n",
            "Epoch 86/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1530 - accuracy: 0.9517\n",
            "Epoch 87/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1543 - accuracy: 0.9379\n",
            "Epoch 88/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.9517\n",
            "Epoch 89/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1510 - accuracy: 0.9379\n",
            "Epoch 90/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1414 - accuracy: 0.9448\n",
            "Epoch 91/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1458 - accuracy: 0.9655\n",
            "Epoch 92/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1770 - accuracy: 0.9241\n",
            "Epoch 93/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1305 - accuracy: 0.9655\n",
            "Epoch 94/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9448\n",
            "Epoch 95/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1459 - accuracy: 0.9379\n",
            "Epoch 96/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1278 - accuracy: 0.9724\n",
            "Epoch 97/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9655\n",
            "Epoch 98/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9793\n",
            "Epoch 99/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9655\n",
            "Epoch 100/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9793\n",
            "Epoch 101/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1235 - accuracy: 0.9724\n",
            "Epoch 102/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9655\n",
            "Epoch 103/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.9724\n",
            "Epoch 104/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9586\n",
            "Epoch 105/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.9793\n",
            "Epoch 106/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1073 - accuracy: 0.9724\n",
            "Epoch 107/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1106 - accuracy: 0.9793\n",
            "Epoch 108/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1096 - accuracy: 0.9862\n",
            "Epoch 109/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1005 - accuracy: 0.9793\n",
            "Epoch 110/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1003 - accuracy: 0.9931\n",
            "Epoch 111/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0956 - accuracy: 0.9793\n",
            "Epoch 112/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0940 - accuracy: 0.9931\n",
            "Epoch 113/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0939 - accuracy: 0.9793\n",
            "Epoch 114/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.9931\n",
            "Epoch 115/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1027 - accuracy: 0.9793\n",
            "Epoch 116/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9793\n",
            "Epoch 117/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9793\n",
            "Epoch 118/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0848 - accuracy: 0.9931\n",
            "Epoch 119/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9931\n",
            "Epoch 120/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.9862\n",
            "Epoch 122/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0749 - accuracy: 0.9931\n",
            "Epoch 125/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0727 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0703 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9931\n",
            "Epoch 130/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.9931\n",
            "Epoch 134/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.9862\n",
            "Epoch 136/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0514 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0488 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0446 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0464 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0378 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# 모델을 설정합니다.\n",
        "model = Sequential()\n",
        "model.add(Dense(24,  input_dim=60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델을 실행합니다.\n",
        "history=model.fit(X_train, y_train, epochs=200, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3mxauzgQKqx",
        "outputId": "2a034fa0-0ca1-4132-bcd6-e92891a75242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7985 - accuracy: 0.8254\n",
            "Test accuracy: 0.8253968358039856\n"
          ]
        }
      ],
      "source": [
        "# 모델을 테스트셋에 적용해 정확도를 구합니다. \n",
        "score=model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpbTQ3iwkL72"
      },
      "source": [
        "학습 데이터셋(X_train과 y_train)을 이용해 200번의 학습을 진행했을 때 모델이 판단한 학습 데이터셋에 대한 정확도와 생성된 모델을 테스트셋을 적용했을 때 보인 정확도가 다르다는 것입니다. 테스트 데이터셋을 적용했을 때의 정확도가 학습 데이터셋를 활용해 생성한 모델이 학습 데이터셋를 가지고 판단한 정확도 보다 낮습니다. \n",
        "\n",
        "\n",
        "머신러인, 딥러닝의 목표는 학습 데이터셋에서만 잘 동작하는 모델을 만드는 것이 아니라. 새로운 데이터에 대해 높은 정확도를 안정되게 보여주는 모델을 만드는 것이 목표입니다. 어떻게 하면 그러한 모델을 만들 수 있을까요? 모델 성능의 향상을 위한 방법에는 크게 데이터를 보강하는 방법과 알고리즘을 최적화 하는 방법이 있습니다. \n",
        "\n",
        "데이터를 이용해 성능을 향상시키려면 우선 충분한 데이터를 가져와 구가하면 됩니다. 많이 알려진 아래 그래프는 특히 딥러닝의 경우 샘플 수가 많을 수록 성능이 좋아짐을 보여줍니다.  \n",
        "\n",
        "<br><center>\n",
        "(그림. 13-6) 데이터의 증가와 딥러닝, 머신러닝 성능의 상관관계<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1WELFk7zpYiBYWmMnd7eJ7tlGXR36xrTL\" width=400>\n",
        "</center><br>  \n",
        "\n",
        "하지만 데이터를 추가하는 것 자체가 어렵거나 데이터 추가만으로 성능 개선에 한계가 있을 수 있습니다. 딸서 가지고 있는 데이터를 적절히 보완해 주는 방법을 사용합니다. 예를 들어 사진의 경우 크기를 확대/축소한 것을 데이터 셋에 추가해 보거나 위 아래로 조금씩 움직인 사진을 데이터셋에 추가하는 것입니다.(이 내용은 20장에서 다룹니다) 테이블형 테이터의 경우 너무 크거나 낮은 이상치가 모델에 영향을 줄 수 없도록 크기를 적절히 조절할 수 있습니다. 시그모이드 함수를 사용해 전체를 0~1사이의 값으로 변환하는 것이 좋은 예입니다. 또 교차 검증 방법을 사용해서 가지고 있는 데이터를 충분히 이용하는 방법도 있습니다. 이는 잠시 후에 설명할 것입니다. \n",
        "\n",
        "다음으로 알고리즘을 이용해 성능을 향상하는 방법은 먼저 다른 구조로 모델을 바꾸어 가며 최적의 구조를 찾는 것입니다. 예를 들어 은닉층의 개수라든지, 그 안에 들어갈 노드의 수, 최적화 함수의 종류를 바꾸어 보는 것입니다. 앞서 이야기한 바 있지만 딥러닝 설정에 정답은 없습니다. 자신의 상황에 맞는 구조를 계속해서 테스트 해보며 찾는 것이 중요합니다. 그리고 데이터에 따라서는 딥러닝이 아닌 랜덤 포레스트, XGBoost, SVM 등 다른 알고리즘이 더 좋은 결과를 보일 때도 있습니다. 일반적인 머신 러닝과 딥러닝을 합해서 더 좋은 결과를 만드는 것도 가능하지요. 많은 경험을 통해 최적의 성능을 보이는 모델을 만드는 것이 중요합니다. \n",
        "\n",
        "**이제 현재 모델을 저장하고 불러오는 방법에 대해 알아보겠습니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLvs4-3YQKqx"
      },
      "source": [
        "## 4. 모델 저장과 재사용\n",
        "학습이 끝난 후 지금 만든 모델을 저장하면 언제든지 이를 불러와 다시 사용할 수 있습니다. 학습 결과를 저장하려면 ```model.save()```함수를 이용해 모델을 저장할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg8aMT6SQKqy"
      },
      "outputs": [],
      "source": [
        "# 모델 이름과 저장할 위치를 함께 지정합니다. \n",
        "model.save('./data/model/my_model.hdf5') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FglmkgWiq0PG"
      },
      "source": [
        "hdf5 파일 포멧은 주로 과학 기술 데이터 작업에서 사용되는데, 크고 복잡한 데이터를 저장하는데 사용됩니다. 이를 다시 불러오려면 케라스 API의 ```load_model()```함수를 사용합니다. 앞서 ```Sequential()``` 함수를 불러온 모델 클래스 안에 함께 들어 있으므로 ```Sequential``` 뒤에 ```load_model```을 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTYfJ1ImVm38"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjy9EyMzrtfP"
      },
      "source": [
        "좀전에 만든 모델을 메모리에서 삭제하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuXXxhc8QKqy"
      },
      "outputs": [],
      "source": [
        "# 테스트를 위해 조금 전 사용한 모델을 메모리에서 삭제합니다.\n",
        "del model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtUVhBw0rzCg"
      },
      "source": [
        "저장된 모델을 불러 옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j7PwJfoQKqy",
        "outputId": "d3edebbb-d2ed-41cb-94db-ca84d12c7a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7985 - accuracy: 0.8254\n",
            "Test accuracy: 0.8253968358039856\n"
          ]
        }
      ],
      "source": [
        "# 모델을 새로 불러옵니다.\n",
        "model = load_model('./data/model/my_model.hdf5') \n",
        "\n",
        "# 불러온 모델을 테스트셋에 적용해 정확도를 구합니다. \n",
        "score=model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr5gjdrMr4X4"
      },
      "source": [
        "테스트 데이터셋을 가지고 정확도 검사를 다시 해봤습니다. 이전과 같은 결과를 얻은 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a00yiT-KtEYE"
      },
      "source": [
        "### [과제]\n",
        "과제 1 - 데이터셋의 65%를 학습 데이터셋으로하고 35%를 데트트 데이터셋으로 나누어서 위 과정을 수행하십시요. 생성된 모델의 학습데이터셋에 대한 정확도와 테스트 데이터셋에 대한 정확도를 제시하십시요.\n",
        "\n",
        "과제 2 - 데이터셋의 80%를 학습 데이터셋으로하고 20%를 데트트 데이터셋으로 나누어서 위 과정을 수행하십시요. 생성된 모델의 학습데이터셋에 대한 정확도와 테스트 데이터셋에 대한 정확도를 제시하십시요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSmg1OtdtnVi",
        "outputId": "1344ced1-2b9b-46c0-c53f-285e8fe18376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.5556\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.5852\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6674 - accuracy: 0.5704\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6606 - accuracy: 0.5926\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.5926\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.6074\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.6403 - accuracy: 0.6074\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.6370\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6247 - accuracy: 0.6444\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.6444\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.6963\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6014 - accuracy: 0.7111\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5887 - accuracy: 0.7704\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.7111\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5710 - accuracy: 0.7481\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.7778\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5501 - accuracy: 0.8000\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5401 - accuracy: 0.8222\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7852\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5220 - accuracy: 0.7852\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.8222\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.8296\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.8074\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8074\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.8222\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.8370\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.8148\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4536 - accuracy: 0.8593\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.8370\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.8296\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.8519\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.8370\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.8074\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8444\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4169 - accuracy: 0.8370\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.8222\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8370\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8741\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8296\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8296\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8370\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3801 - accuracy: 0.8815\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8593\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.8741\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8741\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3642 - accuracy: 0.8667\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8815\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.8667\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8444\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8741\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8593\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8667\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3498 - accuracy: 0.8593\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8296\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3447 - accuracy: 0.8593\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8593\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8519\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8593\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.8741\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.9185\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8741\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8963\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3005 - accuracy: 0.8963\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8963\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.8963\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.8963\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2904 - accuracy: 0.9037\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.9185\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.8815\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.9037\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.9111\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.9111\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.9333\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.9037\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.9333\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.9259\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.9185\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2537 - accuracy: 0.9037\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2510 - accuracy: 0.9185\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2495 - accuracy: 0.9259\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2521 - accuracy: 0.9185\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2430 - accuracy: 0.9333\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2431 - accuracy: 0.9037\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9481\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9407\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2295 - accuracy: 0.9259\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9333\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2254 - accuracy: 0.9333\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2218 - accuracy: 0.9185\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9556\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9333\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9333\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2116 - accuracy: 0.9481\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9185\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.9407\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9407\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2024 - accuracy: 0.9407\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9556\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1950 - accuracy: 0.9481\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1947 - accuracy: 0.9556\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1912 - accuracy: 0.9333\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9259\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1887 - accuracy: 0.9481\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1817 - accuracy: 0.9481\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1825 - accuracy: 0.9481\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1794 - accuracy: 0.9556\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1760 - accuracy: 0.9407\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1813 - accuracy: 0.9407\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1729 - accuracy: 0.9630\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1737 - accuracy: 0.9481\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1671 - accuracy: 0.9556\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1666 - accuracy: 0.9556\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9630\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9630\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1588 - accuracy: 0.9630\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1547 - accuracy: 0.9630\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1615 - accuracy: 0.9481\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1564 - accuracy: 0.9556\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1511 - accuracy: 0.9481\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9556\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1446 - accuracy: 0.9630\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1460 - accuracy: 0.9556\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1449 - accuracy: 0.9556\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.9556\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1409 - accuracy: 0.9630\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1411 - accuracy: 0.9630\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.9556\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1337 - accuracy: 0.9556\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9630\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9556\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9556\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9704\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1478 - accuracy: 0.9481\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9630\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9556\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1263 - accuracy: 0.9630\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9556\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9630\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1184 - accuracy: 0.9778\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1198 - accuracy: 0.9704\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 0.9630\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9704\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.9630\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1062 - accuracy: 0.9704\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1087 - accuracy: 0.9778\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9481\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1144 - accuracy: 0.9704\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9630\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1024 - accuracy: 0.9778\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1014 - accuracy: 0.9704\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0994 - accuracy: 0.9852\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9704\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1001 - accuracy: 0.9778\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9778\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0941 - accuracy: 0.9778\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0937 - accuracy: 0.9778\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0956 - accuracy: 0.9630\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.9778\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9704\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1054 - accuracy: 0.9556\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9704\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0890 - accuracy: 0.9778\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9778\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0829 - accuracy: 0.9852\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0861 - accuracy: 0.9704\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0868 - accuracy: 0.9778\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9778\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0820 - accuracy: 0.9778\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0863 - accuracy: 0.9704\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0877 - accuracy: 0.9704\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0793 - accuracy: 0.9852\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0789 - accuracy: 0.9704\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9778\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9852\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9852\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9852\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9852\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0653 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9926\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9852\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0637 - accuracy: 0.9852\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 0.9852\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0604 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0611 - accuracy: 0.9926\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0514 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 1.0000\n",
            "------------------------------------------------------------------------\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7277 - accuracy: 0.8082\n",
            "Test accuracy: 0.8082191944122314\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 광물 데이터를 불러옵니다.\n",
        "df = pd.read_csv('./data/sonar3.csv', header=None)\n",
        "# 음파 관련 속성을 X로, 광물의 종류를 y로 저장합니다.\n",
        "X = df.iloc[:,0:60]\n",
        "y = df.iloc[:,60]\n",
        "\n",
        "# 학습셋과 테스트셋을 구분합니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, shuffle=True)\n",
        "\n",
        "# 모델을 설정합니다.\n",
        "model = Sequential()\n",
        "model.add(Dense(24,  input_dim=60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델을 실행합니다.\n",
        "history=model.fit(X_train, y_train, epochs=200, batch_size=10)\n",
        "\n",
        "print('---'*24)\n",
        "# 모델을 테스트셋에 적용해 정확도를 구합니다. \n",
        "score=model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lupcBUG6wA7c",
        "outputId": "25b88432-ca09-4569-ff36-4bd48b11e532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.7123 - accuracy: 0.4819\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5060\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6760 - accuracy: 0.5361\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6613 - accuracy: 0.5843\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6498 - accuracy: 0.6205\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6325\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6265 - accuracy: 0.7108\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6116 - accuracy: 0.7048\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5981 - accuracy: 0.7530\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7892\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.7470\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5550 - accuracy: 0.7651\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5412 - accuracy: 0.7831\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5226 - accuracy: 0.8012\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7892\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.8193\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.8072\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7892\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4576 - accuracy: 0.8253\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.8253\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.8193\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.8072\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.8012\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8133\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4064 - accuracy: 0.8253\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8434\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8494\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8373\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.8675\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8313\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8494\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.8735\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8434\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8434\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3332 - accuracy: 0.8735\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8855\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8494\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.8735\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8976\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.8795\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8675\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8916\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.8916\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.9157\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2783 - accuracy: 0.9036\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2726 - accuracy: 0.8976\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.9096\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.9036\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2563 - accuracy: 0.9096\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.9157\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2455 - accuracy: 0.9217\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2471 - accuracy: 0.9096\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.9036\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9277\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.9217\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2388 - accuracy: 0.9096\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2260 - accuracy: 0.9217\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9217\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9096\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9398\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9337\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2054 - accuracy: 0.9337\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2029 - accuracy: 0.9337\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9398\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9398\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1992 - accuracy: 0.9277\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.9277\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1924 - accuracy: 0.9277\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1872 - accuracy: 0.9337\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1816 - accuracy: 0.9578\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1784 - accuracy: 0.9458\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9398\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1730 - accuracy: 0.9458\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1698 - accuracy: 0.9518\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1738 - accuracy: 0.9458\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1672 - accuracy: 0.9518\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1666 - accuracy: 0.9578\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1647 - accuracy: 0.9518\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.9578\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1585 - accuracy: 0.9518\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1513 - accuracy: 0.9578\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.9518\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1486 - accuracy: 0.9578\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9518\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1627 - accuracy: 0.9578\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9578\n",
            "Epoch 87/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1483 - accuracy: 0.9578\n",
            "Epoch 88/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1409 - accuracy: 0.9518\n",
            "Epoch 89/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1430 - accuracy: 0.9819\n",
            "Epoch 90/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9639\n",
            "Epoch 91/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.9398\n",
            "Epoch 92/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9639\n",
            "Epoch 93/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9639\n",
            "Epoch 94/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9578\n",
            "Epoch 95/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1249 - accuracy: 0.9699\n",
            "Epoch 96/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.9578\n",
            "Epoch 97/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1192 - accuracy: 0.9639\n",
            "Epoch 98/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1184 - accuracy: 0.9639\n",
            "Epoch 99/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1188 - accuracy: 0.9639\n",
            "Epoch 100/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1200 - accuracy: 0.9819\n",
            "Epoch 101/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.9699\n",
            "Epoch 102/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9699\n",
            "Epoch 103/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1170 - accuracy: 0.9759\n",
            "Epoch 104/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1119 - accuracy: 0.9639\n",
            "Epoch 105/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 0.9819\n",
            "Epoch 106/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1146 - accuracy: 0.9578\n",
            "Epoch 107/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1051 - accuracy: 0.9699\n",
            "Epoch 108/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9699\n",
            "Epoch 109/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1048 - accuracy: 0.9699\n",
            "Epoch 110/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1018 - accuracy: 0.9699\n",
            "Epoch 111/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0982 - accuracy: 0.9759\n",
            "Epoch 112/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9759\n",
            "Epoch 113/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1012 - accuracy: 0.9819\n",
            "Epoch 114/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0999 - accuracy: 0.9759\n",
            "Epoch 115/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0953 - accuracy: 0.9759\n",
            "Epoch 116/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9759\n",
            "Epoch 117/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0903 - accuracy: 0.9819\n",
            "Epoch 118/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0912 - accuracy: 0.9699\n",
            "Epoch 119/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.9759\n",
            "Epoch 120/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0872 - accuracy: 0.9759\n",
            "Epoch 121/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0883 - accuracy: 0.9699\n",
            "Epoch 122/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0891 - accuracy: 0.9699\n",
            "Epoch 123/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0835 - accuracy: 0.9759\n",
            "Epoch 124/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9759\n",
            "Epoch 125/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0821 - accuracy: 0.9819\n",
            "Epoch 126/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9759\n",
            "Epoch 127/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0826 - accuracy: 0.9819\n",
            "Epoch 128/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9880\n",
            "Epoch 129/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0788 - accuracy: 0.9819\n",
            "Epoch 130/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9819\n",
            "Epoch 131/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9819\n",
            "Epoch 132/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9699\n",
            "Epoch 133/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9819\n",
            "Epoch 134/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0765 - accuracy: 0.9819\n",
            "Epoch 135/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9819\n",
            "Epoch 136/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9699\n",
            "Epoch 137/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0678 - accuracy: 0.9819\n",
            "Epoch 138/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9819\n",
            "Epoch 139/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0676 - accuracy: 0.9880\n",
            "Epoch 140/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0678 - accuracy: 0.9880\n",
            "Epoch 141/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0669 - accuracy: 0.9880\n",
            "Epoch 142/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0652 - accuracy: 0.9819\n",
            "Epoch 143/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9819\n",
            "Epoch 144/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9880\n",
            "Epoch 145/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9880\n",
            "Epoch 146/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0703 - accuracy: 0.9819\n",
            "Epoch 147/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0760 - accuracy: 0.9759\n",
            "Epoch 148/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0642 - accuracy: 0.9759\n",
            "Epoch 149/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0634 - accuracy: 0.9880\n",
            "Epoch 150/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 0.9940\n",
            "Epoch 151/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9880\n",
            "Epoch 152/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9880\n",
            "Epoch 153/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9880\n",
            "Epoch 154/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9880\n",
            "Epoch 155/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9819\n",
            "Epoch 156/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9880\n",
            "Epoch 157/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9880\n",
            "Epoch 158/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 0.9880\n",
            "Epoch 159/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9819\n",
            "Epoch 160/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9940\n",
            "Epoch 161/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0536 - accuracy: 0.9819\n",
            "Epoch 162/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0575 - accuracy: 0.9880\n",
            "Epoch 163/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9940\n",
            "Epoch 164/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0487 - accuracy: 0.9940\n",
            "Epoch 165/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9940\n",
            "Epoch 166/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9880\n",
            "Epoch 167/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9940\n",
            "Epoch 168/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9940\n",
            "Epoch 169/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9880\n",
            "Epoch 170/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9940\n",
            "Epoch 171/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9880\n",
            "Epoch 172/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9880\n",
            "Epoch 173/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.9940\n",
            "Epoch 174/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9940\n",
            "Epoch 175/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9940\n",
            "Epoch 176/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9940\n",
            "Epoch 177/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9940\n",
            "Epoch 178/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0407 - accuracy: 0.9940\n",
            "Epoch 179/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 0.9940\n",
            "Epoch 180/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9940\n",
            "Epoch 181/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9940\n",
            "Epoch 182/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9940\n",
            "Epoch 183/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 0.9940\n",
            "Epoch 184/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0382 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9940\n",
            "Epoch 186/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9940\n",
            "Epoch 187/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9940\n",
            "Epoch 188/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9940\n",
            "Epoch 189/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 0.9940\n",
            "Epoch 192/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9940\n",
            "Epoch 196/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.9940\n",
            "Epoch 199/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 1.0000\n",
            "------------------------------------\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.5215 - accuracy: 0.7619\n",
            "Test accuracy: 0.761904776096344\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 광물 데이터를 불러옵니다.\n",
        "df = pd.read_csv('./data/sonar3.csv', header=None)\n",
        "# 음파 관련 속성을 X로, 광물의 종류를 y로 저장합니다.\n",
        "X = df.iloc[:,0:60]\n",
        "y = df.iloc[:,60]\n",
        "\n",
        "# 학습셋과 테스트셋을 구분합니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "# 모델을 설정합니다.\n",
        "model = Sequential()\n",
        "model.add(Dense(24,  input_dim=60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델을 실행합니다.\n",
        "history=model.fit(X_train, y_train, epochs=200, batch_size=10)\n",
        "\n",
        "print('---'*24)\n",
        "# 모델을 테스트셋에 적용해 정확도를 구합니다. \n",
        "score=model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uvgQSD4QKqz"
      },
      "source": [
        "## 5. k겹 교차 검증\n",
        "\n",
        "데이터가 충분히 많아야 모델 성능을 향상된다고 앞서 말했습니다. 이는 학습과 테스트를 위한 데이터를 충분히 확보할수록 세상에 나왔을 때 더 잘 동작하기 때문입니다. 하지만 실제 프로젝트에서는 데이터를 확보하는 것이 쉽지 않거나 많은 비용이 발생하는 경우도 있습니다. 따라서 가지고 있는 데이터를 십분 활용하는 것이 중요합니다. 특히 학습셋을 70%, 테스트셋을 30%로 설정할 경우 30%의 테스트셋은 학습에 이용할 수 없다는 단점이 있습니다.\n",
        "\n",
        "이를 해결하기 위해 고안된 방법이 k겹 교차 검증(k-fold cross validation)입니다. k겹 교차 검증이란 먼저 데이터셋을 k 개로 나누고 그중 하나를 테스트셋으로 사용하고 테스트셋으로 선정하지 않은 나머지 데이터셋를 모두 합해서 학습셋으로 사용하여 정확도를 구합니다. 다시 k 개의 데이테셋에서 테스트셋으로 선택되지 않은 데이터셋을 테스트셋으로 사용하고 나머지 테이터셋을 모아 학습 데이터셋으로 사용하여 정확도를 구합니다. k개의 데이터셋을  다 한번씩 테스트 셋으로 두고 정확도를 구해서 얻은 k개의 정확도의 평균을 구해 최종 정확도를 판단합니다.\n",
        "\n",
        "이렇게 하면 가지고 있는 데이터의 100%를 학습셋으로 사용할 수 있고 또 동시에 테스트셋으로도 사용할 수 있습니다. 예를 들어 5겹 교차 검증(5-fold cross validation)의 예가 그림(13-7)에 설며되어 있습니다.   \n",
        "<br><center>\n",
        "(그림. 13-7) 5겹 교차 검증 방법<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1UXPDWkDSPC0hhLCwxNwgWPuSfVPHw5Xm\" width=500>\n",
        "</center><br>  \n",
        "\n",
        "이제 초음파 광물 예측 예제를 통해 5겹 교차 검증을 실히새 보겠습니다. 데이터를 원하는 수만큼 나우어 각각 학습셋과 테스트 셋으로 사용하게 하는 함수는 사이킷런 라이브러리의 ```KFold()```함수입니다. 실습 코드에서 ```KFold()```를 활용하는 부부만 뽑아 보며 다음과 같습니다. \n",
        "\n",
        "```\n",
        "k=5\n",
        "kfold = KFold(n_splits=k, shuffle=True)\n",
        "acc_score=[]\n",
        "\n",
        "for tranin_index, test_index in kfold.split(X):\n",
        "   X_train, X_text = X.iloc[train_index, :], X.iloc[test_index,:]\n",
        "   y_train, y_text = y.iloc[trian_index], y.iloc[test_index]\n",
        "```\n",
        "\n",
        "데이터셋을 몇개로 나눌 것인지 정해서 ```k```변수에 할당합니다. 사이킷런의 ```KFold()``` 함수를 불러 옵니다. ```shuffle```에 ```True```를 할당하면 데이터셋을 섞습니다. _k_번의 정확도 계산 결과(정확도 값)를 ```acc_score``` 리스트에 할당할 예정입니다. ```split()``` 함수에 의해 k개의 학습셋과 테스트 셋으로 분리되며 ```for``` 문에 의해 _k_번 반복됩니다. \n",
        "\n",
        "반복되는 매 학습 과정 마다 정확도를 구해 다음과같이 ```acc_score``` 리스트에 붙입니다.\n",
        "\n",
        "```\n",
        "accuracy = model.evaluate(X_test,y_test)   # 정확도를 구합니다.\n",
        "acc_score.append(accuracy[1])              # acc_score 리스트에 저장합니다.\n",
        "```\n",
        "\n",
        "_k_번의 학습이 끝나면 각 정확도를 취합해 모델 성능 평가를 합니다. 아래에는 완성된 코드를 보입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPyhpZRLQKqz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 깃허브에 준비된 데이터를 가져옵니다. 앞에서 이미 데이터를 가져왔으므로 추석 처리합니다. 3번 예제만 별도 실행 시 주석을 해제하여 실습하세요.\n",
        "# !git clone https://github.com/taehojo/data.git\n",
        "\n",
        "# 광물 데이터를 불러옵니다.\n",
        "df = pd.read_csv('./data/sonar3.csv', header=None)\n",
        "\n",
        "# 음파 관련 속성을 X로, 광물의 종류를 y로 저장합니다.\n",
        "X = df.iloc[:,0:60]\n",
        "y = df.iloc[:,60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO--g53mQKq0",
        "outputId": "85310807-18e8-4410-da8e-f7ae7988a743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9458 - accuracy: 0.7857\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4337 - accuracy: 0.8571\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5337 - accuracy: 0.8810\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6926 - accuracy: 0.7561\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4666 - accuracy: 0.8293\n",
            "정확도: [0.7857142686843872, 0.8571428656578064, 0.8809523582458496, 0.7560975551605225, 0.8292682766914368]\n",
            "정확도 평균: 0.8218350648880005\n"
          ]
        }
      ],
      "source": [
        "# 몇 겹으로 나눌 것인지를 정합니다. \n",
        "k=5\n",
        "\n",
        "# KFold 함수를 불러옵니다. 분할하기 전에 샘플이 치우치지 않도록 섞어 줍니다.\n",
        "kfold = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# 정확도가 채워질 빈 리스트를 준비합니다.\n",
        "acc_score = []\n",
        "\n",
        "# 모델 구조 생성\n",
        "def model_fn():\n",
        "    model = Sequential() # 딥러닝 모델의 구조를 시작합니다.\n",
        "    model.add(Dense(24, input_dim=60, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# K겹 교차 검증을 이용해 k번의 학습을 실행합니다. \n",
        "for train_index , test_index in kfold.split(X):  # for 문에 의해서 k번 반복합니다. spilt()에 의해 k개의 학습셋, 테스트셋으로 분리됩니다.\n",
        "    X_train , X_test = X.iloc[train_index,:], X.iloc[test_index,:]  \n",
        "    y_train , y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model = model_fn()\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    history=model.fit(X_train, y_train, epochs=200, batch_size=10, verbose=0) \n",
        "    \n",
        "    accuracy = model.evaluate(X_test, y_test)[1]  # 정확도를 구합니다.\n",
        "    acc_score.append(accuracy)  # 정확도 리스트에 저장합니다.\n",
        "\n",
        "# k번 실시된 정확도의 평균을 구합니다.\n",
        "avg_acc_score = sum(acc_score)/k\n",
        "\n",
        "# 결과를 출력합니다.\n",
        "print('정확도:', acc_score)\n",
        "print('정확도 평균:', avg_acc_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaNAf_m432x7"
      },
      "source": [
        "학습이 진행되는 과정을 화면에 출력되지 않게 하려고 ```model.fit()``` 함수의 파라메타 ```verbose```에 0을 할당했습니다.\n",
        "\n",
        "🚀 잠깐만요.  \n",
        "텐서플로 함수가 for문에 포함되는 경우 다음과 같은 WARNING 메시지가 나오는 경우가 있습니다. 텐서프로 구동에는 문제가 없으므로 그냥 진해하면 됩니다.  \n",
        "WARNING:tensorflow: 5 out of the last 9 call to <function Model.make_test_function.<locals>.test_function at ....> triggered tf.function retracing...\n",
        "\n",
        "\n",
        "<br>\n",
        "이렇게 해서 가지고 있는 데이터를 모두 사용해 학습과 테스트를 진행했습니다. 이제 다음 장에서 학습 과정을 시각화해 보는 방법과 학습을 몇 번 반복할지(epochs) 스스로 판단하게 하는 방법 등을 알아보며 모델 성능을 향상시켜 보겠습니다. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d974c4b7c9167b3c2df7ef2f40c9ab050914ef25a32a81d1263224e0dc77032"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
