{"cells":[{"cell_type":"markdown","metadata":{"id":"XsalpJ_KLSSv"},"source":["# 17장 딥러닝을 이용한 자연어 처리\n","\n","- 자연어(Natural Language)란 우리가 평소에 말하는 음성이나 텍스트를 의미.\n","- 자연어 처리(Natural Language Processing)는 음성이나 텍스트를 컴퓨터가 인식하고 처리"]},{"cell_type":"markdown","metadata":{"id":"dyGt_ra8LSSz"},"source":["## 1. 텍스트의 토큰화\n"," 먼저 해야할 일은 텍스트를 잘게 나누는 것입니다. 입력할 텍스트가 준비되면 이를 단어별, 문장별, 형태소별로 나눌 있는데, 이렇게 작게 나누어진 하나의 단위를 **토큰(token)**이라고 함. 예를 들어 다음과 같은 문장이 주어졌다고 가정해 보자.  \n","```\"해보지 않으면 해낼 수 없다\"```\n","\n","케라스가 제공하는 text 모듈의 ```text_to_word_sequence()```함수를 사용하면 문장을 단어 단위로 나눌 수 있음. 전처리할 덱스트를 지정한 후 다음과 같이 토큰화 함.\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["# 전처리 과정 연습\n","\n","# 케라스의 텍스트 전처리와 관련한 함수 중 text_to_word_sequence 함수를 불러옵니다.\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","text = '해보지 않으면 해낼 수 없다'\n","\n","result = text_to_word_sequence(text)\n","\n","print('원문 :', text)\n","print('토큰화된 결과 :', result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9eqHih3WD5q1","executionInfo":{"status":"ok","timestamp":1666101795485,"user_tz":-540,"elapsed":3001,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"2ac33ba5-d175-4f08-e50d-e4aefac1baff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["원문 : 해보지 않으면 해낼 수 없다\n","토큰화된 결과 : ['해보지', '않으면', '해낼', '수', '없다']\n"]}]},{"cell_type":"markdown","source":["### [연습장]"],"metadata":{"id":"J6BDzptmPAZP"}},{"cell_type":"code","source":["print( type(result) )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjYcmeMcPDNI","executionInfo":{"status":"ok","timestamp":1666101801025,"user_tz":-540,"elapsed":11,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"386a0dc2-9df5-470d-86d3-cc5689eb055f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n"]}]},{"cell_type":"code","source":["text = '한글 분석을 시작합니다. 잘 되겠죠?'\n","\n","result = text_to_word_sequence(text)\n","\n","print('원문 :', text)\n","print('토큰화된 결과 :', result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3WtdECYZuoE","executionInfo":{"status":"ok","timestamp":1666101804471,"user_tz":-540,"elapsed":16,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"b499b751-4dd7-4b78-d37e-0bec42c35996"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["원문 : 한글 분석을 시작합니다. 잘 되겠죠?\n","토큰화된 결과 : ['한글', '분석을', '시작합니다', '잘', '되겠죠']\n"]}]},{"cell_type":"markdown","source":["이렇게 **주어진 텍스트를 단어 단위로 쪼개고 나면 이를 이용해 여러 가지를 할 수 있습니다.** 예를 들어 각 단어가 몇 번이나 중복해서 쓰였는지 알 수 있습니다. 단어의 빈도수를 알면 텍스트에서 중요한 역활을 하는 단어를 파악할 수 있겠지요. 따라서 **텍스틀 단어 단위로 쪼개는 것은 가장 많이 쓰이는 전처리 과정.** \n","\n","Bag-of-Words라는 방법이 이러한 전처리를 일컫는 말인데, '단어의 가방(bag of words)'이라는 뜻으로 같은 단어끼리 각각의 가방에 담은 후에 각 가방에 단어가 몇개 들어 있는지 세는 방법입니다. (강사-단어 출현 빈도수를 파악) \n","\n","\n","예를 들어 다음과 같은 세 개의 문장이 있다고 합시다.\n","```\n","먼저 텍스트의 각 단어를 나누어 토큰화합니다.\n","텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.\n","토큰화한 결과는 딥러닝에서 사용할 수 있습니다.\n","```\n"],"metadata":{"id":"efd5y2GpEbKR"}},{"cell_type":"markdown","source":["케라스의 ```Tokenizer()``` 함수를 사용하면 **단어의 빈도수**를 쉽게 계산할 수 있습니다. 다음 코드는 위 제시한 세 문장의 단어를 빈도수로 다시 정리하는 코드입니다."],"metadata":{"id":"smxWOqkAudqZ"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","#전처리하려는 세 개의 문서(document)을 docs라는 리스트에 저장합니다.\n","docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다. 이 문장은 어쩔라고.',\n","        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.'\n","        ]\n","# Tokenizer()\n","#  - 전처리 과정을 수행할 객체 반환\n","token=Tokenizer()\n","token.fit_on_texts(docs)  # 문서(들)을 토큰화함.\n","\n","print('단어 카운트:\\n', token.word_counts)   # 각 단어(token)이 전체 문서에서 몇 번 나타나는지 빈도 정보"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJpUwjvwGRyd","executionInfo":{"status":"ok","timestamp":1666102383815,"user_tz":-540,"elapsed":9,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"7b30509b-e564-4d93-d890-874dcf79fdf0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 카운트:\n"," OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('이', 1), ('문장은', 1), ('어쩔라고', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n"]}]},{"cell_type":"markdown","source":["```token.word_counts```에는 각 단어가 몇번 나타나는지 즉 단어 별 나타난 빈도수를 해당 단어와 같이 포함하고 있습니다."],"metadata":{"id":"FUxBP2nSP7-7"}},{"cell_type":"code","source":["# document_count 속성에서는 총 몇 개의 문서가 들어 있는지 알 수 있음.\n","print('문서 카운트 : ', token.document_count )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-tcy1wxcLZl","executionInfo":{"status":"ok","timestamp":1666102387211,"user_tz":-540,"elapsed":13,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"96fbbe6a-7985-4aa0-8b57-48a4e1e1ac6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["문서 카운트 :  3\n"]}]},{"cell_type":"markdown","source":["#### [실습]"],"metadata":{"id":"Bwanp1taQQZ8"}},{"cell_type":"code","source":["#전처리하려는 세 개의 문장을 docs라는 리스트에 저장합니다.\n","docs = ['난 난 꿈이 있었죠. 버려지고 찢겨 남루하여도 내 가슴 깊숙이 보물과 같이 간직했던 꿈. 혹 때론 누군가가 뜻 모를 비웃음 내 등뒤에 흘릴때도 난 참아야 했죠.',\n","        '참을 수 있었죠. 그 날을 위해 늘 걱정하듯 말하죠. 헛된 꿈은 독이라고 세상은 끝이 정해진 책처럼 이미 돌이킬 수 없는 현실이라고 그래요 난 난 꿈이 있어요.',\n","         '그 꿈을 믿어요. 나를 지켜봐요. 저 차갑게 서 있는 운명이란 벽앞에 당당히 마주칠 수 있어요.',\n","         '언젠가 난 그 벽을 넘고서 저 하늘을 높이 날을 수 있어요. 이 무거운 세상도 나를 묶을 순 없죠.',\n","        '내 삶의 끝에서 나 웃을 그 날을 함께해요.',\n","         '늘 걱정하듯 말하죠. 헛된 꿈은 독이라고. 세상은 끝이 정해진 책처럼 이미 돌이킬 수 없는 현실이라고.',\n","        '그래요. 난 난 꿈이 있어요. 그 꿈을 믿어요 나를 지켜봐요. 저 차갑게 서 있는 운명이란 벽앞에 당당히 마주칠 수 있어요.',\n","      '언젠가 난 그 벽을 넘고서 저 하늘을 높이 날을 수 있어요. 이 무거운 세상도 나를 묶을 순 없죠. 내 삶의 끝에서 나 웃을 그 날을 함께해요.',\n","      '난 난 꿈이 있어요. 그 꿈을 믿어요 나를 지켜봐요'\n","        ]\n","# Tokenizer()를 이용해 전처리 하는 과정\n","token = Tokenizer()\n","token.fit_on_texts(docs)  # 문서(들)을 토큰화 \n","\n","print('단어 카운트:\\n', token.word_counts)\n","\n","# document_count 속성에서 총 몇 개의 문서가 들어 있는지 알 수 있음.\n","print('문서 카운트:\\n', token.document_count )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-JwmtlnQW6f","executionInfo":{"status":"ok","timestamp":1666102439461,"user_tz":-540,"elapsed":340,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"5387878e-9a45-477b-dfa4-823b95df17ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 카운트:\n"," OrderedDict([('난', 11), ('꿈이', 4), ('있었죠', 2), ('버려지고', 1), ('찢겨', 1), ('남루하여도', 1), ('내', 4), ('가슴', 1), ('깊숙이', 1), ('보물과', 1), ('같이', 1), ('간직했던', 1), ('꿈', 1), ('혹', 1), ('때론', 1), ('누군가가', 1), ('뜻', 1), ('모를', 1), ('비웃음', 1), ('등뒤에', 1), ('흘릴때도', 1), ('참아야', 1), ('했죠', 1), ('참을', 1), ('수', 7), ('그', 8), ('날을', 5), ('위해', 1), ('늘', 2), ('걱정하듯', 2), ('말하죠', 2), ('헛된', 2), ('꿈은', 2), ('독이라고', 2), ('세상은', 2), ('끝이', 2), ('정해진', 2), ('책처럼', 2), ('이미', 2), ('돌이킬', 2), ('없는', 2), ('현실이라고', 2), ('그래요', 2), ('있어요', 7), ('꿈을', 3), ('믿어요', 3), ('나를', 5), ('지켜봐요', 3), ('저', 4), ('차갑게', 2), ('서', 2), ('있는', 2), ('운명이란', 2), ('벽앞에', 2), ('당당히', 2), ('마주칠', 2), ('언젠가', 2), ('벽을', 2), ('넘고서', 2), ('하늘을', 2), ('높이', 2), ('이', 2), ('무거운', 2), ('세상도', 2), ('묶을', 2), ('순', 2), ('없죠', 2), ('삶의', 2), ('끝에서', 2), ('나', 2), ('웃을', 2), ('함께해요', 2)])\n","문서 카운트:\n","  9\n"]}]},{"cell_type":"markdown","source":["```python\n","token.word_counts      # 단어(token)와 해당 단어(token)가 전체 문서에 나타난 빈도\n","token.document_count   # 총 몇 개의 문서로 구성되어 있는지 파악\n","```"],"metadata":{"id":"64JRNDkdSM3D"}},{"cell_type":"code","source":["# token.word_docs를 통해 각 단어들이 몇 개의 문서에 나오는지 세어서 출력할 수도 있습니다. \n","# 출력 되는 순서는 랜덤\n","print('각 단어가 몇 개의 문서에 포함되어 있는가 : \\n', token.word_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uE2_6tznIJC6","executionInfo":{"status":"ok","timestamp":1666102494595,"user_tz":-540,"elapsed":327,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"0f0dc44d-964a-43e9-b357-c77a213a205a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["각 단어가 몇 개의 문서에 포함되어 있는가 : \n"," defaultdict(<class 'int'>, {'참아야': 1, '가슴': 1, '찢겨': 1, '내': 3, '흘릴때도': 1, '뜻': 1, '난': 6, '남루하여도': 1, '등뒤에': 1, '했죠': 1, '모를': 1, '같이': 1, '혹': 1, '꿈': 1, '깊숙이': 1, '있었죠': 2, '비웃음': 1, '보물과': 1, '버려지고': 1, '누군가가': 1, '꿈이': 4, '간직했던': 1, '때론': 1, '걱정하듯': 2, '말하죠': 2, '현실이라고': 2, '끝이': 2, '책처럼': 2, '그': 7, '위해': 1, '꿈은': 2, '날을': 4, '정해진': 2, '독이라고': 2, '수': 6, '돌이킬': 2, '그래요': 2, '없는': 2, '늘': 2, '세상은': 2, '헛된': 2, '이미': 2, '있어요': 6, '참을': 1, '있는': 2, '서': 2, '지켜봐요': 3, '운명이란': 2, '믿어요': 3, '꿈을': 3, '마주칠': 2, '벽앞에': 2, '저': 4, '당당히': 2, '차갑게': 2, '나를': 5, '순': 2, '높이': 2, '하늘을': 2, '이': 2, '벽을': 2, '세상도': 2, '없죠': 2, '묶을': 2, '무거운': 2, '언젠가': 2, '넘고서': 2, '함께해요': 2, '나': 2, '웃을': 2, '끝에서': 2, '삶의': 2})\n"]}]},{"cell_type":"code","source":["# 각 단어에 매겨진 인덱스 값을 출력하려면 word_index 속성에서 확인 \n","print('각 단어에 매겨진 인덱스 값:\\n', token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjxUHAdII4gy","executionInfo":{"status":"ok","timestamp":1666102520714,"user_tz":-540,"elapsed":338,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"4f1b9931-a7b2-449d-fa88-f144c502b429"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["각 단어에 매겨진 인덱스 값:\n"," {'난': 1, '그': 2, '수': 3, '있어요': 4, '날을': 5, '나를': 6, '꿈이': 7, '내': 8, '저': 9, '꿈을': 10, '믿어요': 11, '지켜봐요': 12, '있었죠': 13, '늘': 14, '걱정하듯': 15, '말하죠': 16, '헛된': 17, '꿈은': 18, '독이라고': 19, '세상은': 20, '끝이': 21, '정해진': 22, '책처럼': 23, '이미': 24, '돌이킬': 25, '없는': 26, '현실이라고': 27, '그래요': 28, '차갑게': 29, '서': 30, '있는': 31, '운명이란': 32, '벽앞에': 33, '당당히': 34, '마주칠': 35, '언젠가': 36, '벽을': 37, '넘고서': 38, '하늘을': 39, '높이': 40, '이': 41, '무거운': 42, '세상도': 43, '묶을': 44, '순': 45, '없죠': 46, '삶의': 47, '끝에서': 48, '나': 49, '웃을': 50, '함께해요': 51, '버려지고': 52, '찢겨': 53, '남루하여도': 54, '가슴': 55, '깊숙이': 56, '보물과': 57, '같이': 58, '간직했던': 59, '꿈': 60, '혹': 61, '때론': 62, '누군가가': 63, '뜻': 64, '모를': 65, '비웃음': 66, '등뒤에': 67, '흘릴때도': 68, '참아야': 69, '했죠': 70, '참을': 71, '위해': 72}\n"]}]},{"cell_type":"markdown","source":["token의 여러 속성\n","```\n","token.word_counts        # 각 단어(token)이 전체 문서에서 몇 번 나타나는지 빈도 정보\n","token.document_count     # 전체 문서에서 총 몇개의 문서가 있는지\n","token.word_docs          # 단어(token)이 몇 개의 문서에서 나타는지 \n","token.word_index         # 전체 문서에서 해당 단어(tokne)와 그 단어의 인텍스 정보\n","```"],"metadata":{"id":"svJDUkP6cvxi"}},{"cell_type":"markdown","source":["### 1절 덱스트의 토근화에 대한 **요점 정리**\n","케라스가 제공하는 기능(함수)를 사용하면 text를 위한 전처리를 쉽게 할 수 있음."],"metadata":{"id":"ijvtmjXlezOo"}},{"cell_type":"markdown","source":["***"],"metadata":{"id":"qjzVZIdoSvrH"}},{"cell_type":"markdown","source":["## 2. 단어의 원-핫 인코딩\n"," 앞서 우리는 문장을 컴퓨터가 알아 들을 수 있게 토큰화하고 단어의 비도수를 확인해 보았습니다. **(강사 - 글자를 그러니까 단어(token)을 숫자화 해야 모델 입력으로 활용할 수 있음.)** 하지만 단순히 단어의 출현 빈도만 가지고 해당 단어가 문장의 어디에서 왔는지, 각 단어의 순서는 어떠했는지 등에 관한 정보를 얻을 수 없습니다.\n","\n"," 단어가 문장의 다른 요소와 어떤 관계를 가지고 있는지 알아보는 방법이 필요합니다. 이러한 기법 중에서 가장 기본적인 방법인 **원-핫 인코딩**을 알아 보겠습니다. (강사-원-핫 인코딩으로 단어가 문장의 다른 요소와 어떤 관계를 가지는 알 수 있나요?) 원-핫 인코딩을 단어를 요소로하는 배열로 적용해보겠습니다. 예를 들어 다음과 같은 문장이 있습니다. \n","```\n","'오랜동안 꿈꾸는 이는 그 꿈을 닮아간다'\n","```\n","각 단어를 모두 0으로 바꾸어 주고 원하는 단어만 1로 바꾸어 주는 것이 원-핫 인코딩이었습니다. 이를 수행하기 위해 먼저 단어 수만큼 0으로 채워진 벡터 공간으로 바꾸면 다음과 같습니다. \n","```\n","[0 0 0 0 0 0 0]\n","```\n","<br><center>\n","<img src=\"https://drive.google.com/uc?id=1q0fo3bYP1jyyXjKew5d29axliCN4rjZB\">\n","</center><br>\n","이제 각 단어가 배열 내에서 해당하는 위치를 1로 바꾸어서 벡터화 할 수 있습니다.\n","```\n","오랜동안 = [ 0 1 0 0 0 0 0]\n","꿈꾸는   = [ 0 0 1 0 0 0 0]\n","이는     = [ 0 0 0 1 0 0 0]\n","그       = [ 0 0 0 0 1 0 0]\n","꿈을     = [ 0 0 0 0 0 1 0]\n","닮아간다 = [ 0 0 0 0 0 0 1]\n","```\n","이러한 과정을 케라스로 구현해 보겠습니다. 먼저 토큰화 함수를 불러와 단어 단위로 토큰화하고 각 단어의 인덱스 값을 출력해 봅시다. "],"metadata":{"id":"aNvXSzrjJRaF"}},{"cell_type":"code","source":["text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FjUXGrKGUxo","executionInfo":{"status":"ok","timestamp":1666104841260,"user_tz":-540,"elapsed":777,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"00347893-ed1a-45a7-b46a-ab54c6e56bb0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"]}]},{"cell_type":"markdown","source":["이제 각 단어를 원-핫 인코딩 방식으로 표현해 보겠습니다. 케라스에서 제공하는 ```Tokenizer```의 ```text_to_sequence()``` 함수를 사용해서 앞서 만들어진 토큰의 인텍스로만 채워진 새로운 배열을 만들어 줍니다."],"metadata":{"id":"5-8-54PWHXZx"}},{"cell_type":"code","source":["x = token.texts_to_sequences([text])\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCcfDq24Ht6W","executionInfo":{"status":"ok","timestamp":1666104848857,"user_tz":-540,"elapsed":12,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"94dfd332-877f-496c-e560-6840fc3e43f7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4, 5, 6]]\n"]}]},{"cell_type":"markdown","source":["이제 1\\~6의 정수로 인텍스되어 있는 것을 0과 1로만 이루어진 배열로 바꾸어 주는 ```to_categorical()```함수를 사용해 워-핫 인코딩 과정을 진행합니다. 배열 맨 앞에 0이 추가됨으로 단어 수보다 1이 더 많게 인텍스 숫자를 잡아 주는 것에 유의하시기 바랍니다."],"metadata":{"id":"6G7VFkxWIFfP"}},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","word_size = len(token.word_index) + 1\n","x = to_categorical(x, num_classes = word_size)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3N2J_smrIgZH","executionInfo":{"status":"ok","timestamp":1666104855295,"user_tz":-540,"elapsed":480,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"371a6e1b-4a81-4b48-f7c6-222b6f92dd6d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"markdown","source":["## 3. 단어 임베딩\n","\n","```\n","model = Sequential()\n","model.add(Embedding(16, 4))\n","```"],"metadata":{"id":"FEwvDW9gJJkl"}},{"cell_type":"markdown","metadata":{"id":"cyovn0KqLSS5"},"source":["## 4.텍스트를 읽고 긍정, 부정 예측하기"]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Embedding\n","from tensorflow.keras.utils import to_categorical\n","from numpy import array"],"metadata":{"id":"3IdKtUECmCDY","executionInfo":{"status":"ok","timestamp":1666105776493,"user_tz":-540,"elapsed":298,"user":{"displayName":"김성필","userId":"07791892494073583801"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"id":"1P3-i12hLSS6","outputId":"4c2d81f4-0cf2-4bb3-8ac2-dceee3b70821","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666105778640,"user_tz":-540,"elapsed":9,"user":{"displayName":"김성필","userId":"07791892494073583801"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"]}],"source":["# 텍스트 리뷰 자료를 지정합니다.\n","docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n","\n","# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n","classes = array([1,1,1,1,1,0,0,0,0,0])\n","\n","# 토큰화 \n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"P5w47DmtLSS6","outputId":"eba26a62-963e-4063-cb58-3fa11ee9a099","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666105785131,"user_tz":-540,"elapsed":330,"user":{"displayName":"김성필","userId":"07791892494073583801"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","리뷰 텍스트, 토큰화 결과:\n"," [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"]}],"source":["x = token.texts_to_sequences(docs)\n","print(\"\\n리뷰 텍스트, 토큰화 결과:\\n\",  x)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"vf3WNVZmLSS7","outputId":"0ed5545d-7120-48bc-8adf-555b42081cd5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666105789574,"user_tz":-540,"elapsed":322,"user":{"displayName":"김성필","userId":"07791892494073583801"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","패딩 결과:\n"," [[ 0  0  1  2]\n"," [ 0  0  0  3]\n"," [ 4  5  6  7]\n"," [ 0  8  9 10]\n"," [ 0 11 12 13]\n"," [ 0  0  0 14]\n"," [ 0  0  0 15]\n"," [ 0  0 16 17]\n"," [ 0  0 18 19]\n"," [ 0  0  0 20]]\n"]}],"source":["# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다.\n","padded_x = pad_sequences(x, 4)  \n","print(\"\\n패딩 결과:\\n\", padded_x)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"HkhKyqFmLSS7","outputId":"33021929-b873-4f72-de9f-db049a75253d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666105793386,"user_tz":-540,"elapsed":321,"user":{"displayName":"김성필","userId":"07791892494073583801"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["21\n"]}],"source":["# 임베딩에 입력될 단어의 수를 지정합니다.\n","# 실제 단어는 20개인데 20개 데이터가 1부터 인텍싱 되기 때문에 인텍스 값은 1, 2, 3, 19, 20이다.\n","# 그런데 파이썬에서 테이터를 처리할 때 인텍스 0부터 고려하기 때문에 인덱스 0의 요소를 고려해서 단어가 21개인 것으로 처리하는 것으로 이해하자. \n","word_size = len(token.word_index) +1\n","print(word_size)"]},{"cell_type":"code","source":["# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n","model = Sequential()\n","\n","# 21차원 벡터를 8차원 벡터로 변환\n","# word_size: 입력될 단어 개수,\n","# 8: 8 차원 데이터 출력\n","# input_length = 4 : 한 번에 입력하는 단어 개수, [0, 0, 1, 2]\n","model.add(Embedding(word_size, 8, input_length=4))     # 이 예제의 경우 하나의 단어를 원핫 인코딩하면 한 단어를 나타내는 벡타는 20차원.  \n","                                                       # 한 단어는 첫번째 요소에 0을 추가한 것을 고려하면 21차원 벡터이다. \n","                                                       # 이 벡터를 8 차원으로 바꾼다. 그리다 한번에 입력하는 단어(token)의 개수가 4개 따라서 4x8 출력\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"metadata":{"id":"hoAF1mDDBf3Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666105798279,"user_tz":-540,"elapsed":569,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"8f8b0c58-eb1e-44b9-d92c-da2a0fdb65d6"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 4, 8)              168       \n","                                                                 \n"," flatten_1 (Flatten)         (None, 32)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 201\n","Trainable params: 201\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","execution_count":38,"metadata":{"id":"2mE5DbHrLSS8","outputId":"ddcd4ee9-c10e-4e55-a0ef-a7cf0a6b3bd2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666105809291,"user_tz":-540,"elapsed":1828,"user":{"displayName":"김성필","userId":"07791892494073583801"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1/1 [==============================] - 1s 697ms/step - loss: 0.6933 - accuracy: 0.4000\n","Epoch 2/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6910 - accuracy: 0.4000\n","Epoch 3/20\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6888 - accuracy: 0.6000\n","Epoch 4/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6865 - accuracy: 0.7000\n","Epoch 5/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6843 - accuracy: 0.8000\n","Epoch 6/20\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6821 - accuracy: 0.9000\n","Epoch 7/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6798 - accuracy: 0.9000\n","Epoch 8/20\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6776 - accuracy: 0.9000\n","Epoch 9/20\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6754 - accuracy: 0.9000\n","Epoch 10/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6731 - accuracy: 0.9000\n","Epoch 11/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6709 - accuracy: 0.9000\n","Epoch 12/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6686 - accuracy: 0.9000\n","Epoch 13/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6664 - accuracy: 0.9000\n","Epoch 14/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6642 - accuracy: 0.9000\n","Epoch 15/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6619 - accuracy: 0.9000\n","Epoch 16/20\n","1/1 [==============================] - 0s 14ms/step - loss: 0.6596 - accuracy: 0.9000\n","Epoch 17/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6574 - accuracy: 0.9000\n","Epoch 18/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6551 - accuracy: 0.9000\n","Epoch 19/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6528 - accuracy: 0.9000\n","Epoch 20/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6505 - accuracy: 0.9000\n","1/1 [==============================] - 0s 128ms/step - loss: 0.6482 - accuracy: 0.9000\n","\n"," Accuracy: 0.9000\n"]}],"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(padded_x, classes, epochs=20)\n","print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x, classes)[1]))"]},{"cell_type":"markdown","source":["***"],"metadata":{"id":"M7DjKnVWTzOb"}},{"cell_type":"markdown","source":["***"],"metadata":{"id":"xu6W4s7OT20e"}},{"cell_type":"markdown","source":["**아래 내용은 무시 - 미완성이기에**"],"metadata":{"id":"nBBuhLMJpBIQ"}},{"cell_type":"markdown","source":["# 단어 임베딩(Word embedding)\n","## 단어 표현(Word Representation)\n","  \"단어 표현\" 표현 분야는 텍스트를 자연어 처리를 위한 모델에 적용할 수 있게 언어적인 특성을 반영해서 단어를 숫자화하는 방법을 찾는 것입니다.\n","\n","  단어를 수치화할 때는 주로 벡터 형태로 수치화 합니다. 따라서 단어 표현을 \"단어 임베딩(word embedding)\" 또는 단어 벡터로 표현하기도 합니다.  \n","\n"," 원-핫 인코딩은 각 단어의 인덱스를 정한 후 각 단어의 벡터에서 그 단어에 해당하는 인텍스 위치에 값을 1로 표현하는 방식입니다.\n","\n"," 예를 들어 '단맛', '짠맛', '매운맛' 이렇게 3개의 단어가 있다고 가정하겠습니다. 그리고 파이썬 리스트 형태로 단어를 보관하고 있습니다.\n","```python\n","['매운맛', '단맛', '짠맛']\n","```\n","\n","'매운맛'이라는 문자열은 파이썬 리스트의 한 요소라고 보면 리스트의 한 요소인 '매운맛'은 인텍스가 0입니다. "],"metadata":{"id":"CdqvL5Vz_zeC"}},{"cell_type":"markdown","source":["아래 코드에서와 같이 적용했을 때, 매운맛의 인덱스는 1이되어 컴퓨터가 아닌 사람에게 조금 더 친숙한 방식의 인덱싱이 됩니다. "],"metadata":{"id":"Bhxr4-o7EYKb"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","text = '매운맛 단맛 짠맛'\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fS-P56UVCKG9","executionInfo":{"status":"ok","timestamp":1665928999251,"user_tz":-540,"elapsed":520,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"73700fb2-8dd3-40c3-92a5-3d3bda97606e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'매운맛': 1, '단맛': 2, '짠맛': 3}\n"]}]},{"cell_type":"markdown","source":["케라스에서 제공하는 ```Tokenizer```의 ```text_to_sequence()```함수를 사용해서 단어의 인덱스만으로 채워진 새로운 배열를 만들겠습니다."],"metadata":{"id":"MN6p4Yu0E7PY"}},{"cell_type":"code","source":["x = token.texts_to_sequences([text])\n","print( type(x) )\n","print(x)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzixyn4HFkdi","executionInfo":{"status":"ok","timestamp":1665929001860,"user_tz":-540,"elapsed":5,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"1601a273-87de-4e20-b396-4d2298091ec9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","[[1, 2, 3]]\n"]}]},{"cell_type":"markdown","source":["```to_categorical()```함수를 이용하여 원-핫 인코딩을 진행하겠습니다."],"metadata":{"id":"TwNW155QGMT-"}},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","x = to_categorical(x, 4)   # 여기서 4의 의미는?\n","print( type(x) )\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GH4XlpBUGXvZ","executionInfo":{"status":"ok","timestamp":1665929015746,"user_tz":-540,"elapsed":742,"user":{"displayName":"김성필","userId":"07791892494073583801"}},"outputId":"64855220-e9f8-44c9-df32-2b7f90a75348"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","[[[0. 1. 0. 0.]\n","  [0. 0. 1. 0.]\n","  [0. 0. 0. 1.]]]\n"]}]},{"cell_type":"markdown","source":["첫 번째 단어 '매운맛'의 인덱스가 1이므로 핫-원 인코딩 결과가 0번 째, 1번 째, 2번 째, 3번 째 중 1번 째 요소를 1로 하고 나머지 요소는 모두 0으로 표현한 것입니다."],"metadata":{"id":"5nWTamsiHE6Y"}}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[{"file_id":"https://github.com/taehojo/deeplearning/blob/master/colab/ch17-colab.ipynb","timestamp":1665592100091}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}